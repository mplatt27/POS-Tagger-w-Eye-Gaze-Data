{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Dataset construction\n",
    "  author: MP\n",
    "  date: 4/7/2021\n",
    "  \n",
    "  This study uses data from the GECO (Ghent Eye Tracking Corpus) which is available freely for use\n",
    "  at: https://expsy.ugent.be/downloads/geco/.\n",
    "  \n",
    "  The following code extracts the features that are used in this study, and averages over word types.\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>LANGUAGE_RANK</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>TRIAL_FIXATION_COUNT</th>\n",
       "      <th>TRIAL_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_ID_WITHIN_TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>WORD_LAST_FIXATION_RUN</th>\n",
       "      <th>WORD_LAST_FIXATION_TIME</th>\n",
       "      <th>WORD_LAST_FIXATION_X</th>\n",
       "      <th>WORD_LAST_FIXATION_Y</th>\n",
       "      <th>WORD_GO_PAST_TIME</th>\n",
       "      <th>WORD_SELECTIVE_GO_PAST_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>1</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>357</td>\n",
       "      <td>115.6</td>\n",
       "      <td>104.6</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>2</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1392</td>\n",
       "      <td>163.8</td>\n",
       "      <td>107</td>\n",
       "      <td>582</td>\n",
       "      <td>296</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>3</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1957</td>\n",
       "      <td>229.4</td>\n",
       "      <td>96.6</td>\n",
       "      <td>1097</td>\n",
       "      <td>565</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>4</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2474</td>\n",
       "      <td>356</td>\n",
       "      <td>103.8</td>\n",
       "      <td>2107</td>\n",
       "      <td>428</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2808</td>\n",
       "      <td>403.2</td>\n",
       "      <td>114.5</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR        GROUP LANGUAGE_RANK LANGUAGE  PART  TRIAL  \\\n",
       "0  pp21  monolingual            L1  English     1      5   \n",
       "1  pp21  monolingual            L1  English     1      5   \n",
       "2  pp21  monolingual            L1  English     1      5   \n",
       "3  pp21  monolingual            L1  English     1      5   \n",
       "4  pp21  monolingual            L1  English     1      5   \n",
       "\n",
       "   TRIAL_FIXATION_COUNT  TRIAL_TOTAL_READING_TIME  WORD_ID_WITHIN_TRIAL  \\\n",
       "0                   115                     25429                     1   \n",
       "1                   115                     25429                     2   \n",
       "2                   115                     25429                     3   \n",
       "3                   115                     25429                     4   \n",
       "4                   115                     25429                     5   \n",
       "\n",
       "  WORD_ID  ... WORD_LAST_FIXATION_RUN WORD_LAST_FIXATION_TIME  \\\n",
       "0   1_5_1  ...                      2                     357   \n",
       "1   1_5_2  ...                      3                    1392   \n",
       "2   1_5_3  ...                      2                    1957   \n",
       "3   1_5_4  ...                      2                    2474   \n",
       "4   1_5_5  ...                      1                    2808   \n",
       "\n",
       "   WORD_LAST_FIXATION_X  WORD_LAST_FIXATION_Y  WORD_GO_PAST_TIME  \\\n",
       "0                 115.6                 104.6                 95   \n",
       "1                 163.8                   107                582   \n",
       "2                 229.4                  96.6               1097   \n",
       "3                   356                 103.8               2107   \n",
       "4                 403.2                 114.5                154   \n",
       "\n",
       "  WORD_SELECTIVE_GO_PAST_TIME WORD_TOTAL_READING_TIME  \\\n",
       "0                          95                   381.0   \n",
       "1                         296                   828.0   \n",
       "2                         565                   565.0   \n",
       "3                         428                   428.0   \n",
       "4                         154                   154.0   \n",
       "\n",
       "  WORD_TOTAL_READING_TIME_% WORD_SPILLOVER  WORD_SKIP  \n",
       "0                    0.0150            0.0        0.0  \n",
       "1                    0.0326            0.0        1.0  \n",
       "2                    0.0222            0.0        1.0  \n",
       "3                    0.0168            0.0        0.0  \n",
       "4                    0.0061            0.0        1.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset with eye tracking data\n",
    "PTH_DATA = 'C:/6120-NLP/project/GECO/MonolingualReadingData.csv'\n",
    "dataset = pd.read_csv(PTH_DATA)\n",
    "\n",
    "# replace - with _ to avoid excel data reformatting\n",
    "def fix_id(w):\n",
    "    w = str(w)\n",
    "    w = w.replace(\"-\", \"_\")\n",
    "    return w\n",
    "\n",
    "dataset['WORD_ID'] = dataset['WORD_ID'].apply(fix_id)\n",
    "\n",
    "# replace null values, which are represented by a dot, with an empty cell\n",
    "# also change values that we need to floats (some are strings)\n",
    "def fix_nulls(c):\n",
    "    if c == \".\":\n",
    "        c = 0\n",
    "    return float(c)\n",
    "\n",
    "dataset['WORD_FIXATION_%'] = dataset['WORD_FIXATION_%'].apply(fix_nulls)\n",
    "dataset['WORD_FIXATION_COUNT'] = dataset['WORD_FIXATION_COUNT'].apply(fix_nulls)\n",
    "dataset['WORD_GAZE_DURATION'] = dataset['WORD_GAZE_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_RUN_COUNT'] = dataset['WORD_RUN_COUNT'].apply(fix_nulls)\n",
    "dataset['WORD_SKIP'] = dataset['WORD_SKIP'].apply(fix_nulls)\n",
    "dataset['WORD_FIRST_FIXATION_DURATION'] = dataset['WORD_FIRST_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_SECOND_FIXATION_DURATION'] = dataset['WORD_SECOND_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_THIRD_FIXATION_DURATION'] = dataset['WORD_THIRD_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_TOTAL_READING_TIME'] = dataset['WORD_TOTAL_READING_TIME'].apply(fix_nulls)\n",
    "dataset['WORD_TOTAL_READING_TIME_%'] = dataset['WORD_TOTAL_READING_TIME_%'].apply(fix_nulls)\n",
    "dataset['WORD_SPILLOVER'] = dataset['WORD_SPILLOVER'].apply(fix_nulls)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>SENTENCE_ID</th>\n",
       "      <th>CHRON_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>PART_OF_SPEECH</th>\n",
       "      <th>CONTENT_WORD</th>\n",
       "      <th>WORD_LENGTH</th>\n",
       "      <th>IA_AREA</th>\n",
       "      <th>IA_TOP</th>\n",
       "      <th>IA_BOTTOM</th>\n",
       "      <th>IA_LEFT</th>\n",
       "      <th>IA_RIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_5_1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>Article</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3087</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>66</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_5_2</td>\n",
       "      <td>1_1</td>\n",
       "      <td>2</td>\n",
       "      <td>intense</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3920</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_5_3</td>\n",
       "      <td>1_1</td>\n",
       "      <td>3</td>\n",
       "      <td>interest</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4410</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>209</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_5_4</td>\n",
       "      <td>1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>Verb</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3969</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>299</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5_5</td>\n",
       "      <td>1_1</td>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "      <td>Preposition</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1421</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>380</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WORD_ID SENTENCE_ID  CHRON_ID      WORD PART_OF_SPEECH  CONTENT_WORD  \\\n",
       "0   1_5_1         1_1         1       The        Article             0   \n",
       "1   1_5_2         1_1         2   intense      Adjective             1   \n",
       "2   1_5_3         1_1         3  interest           Noun             1   \n",
       "3   1_5_4         1_1         4   aroused           Verb             1   \n",
       "4   1_5_5         1_1         5        in    Preposition             0   \n",
       "\n",
       "   WORD_LENGTH  IA_AREA  IA_TOP  IA_BOTTOM  IA_LEFT  IA_RIGHT  \n",
       "0            3     3087      93        142       66       129  \n",
       "1            7     3920      93        142      129       209  \n",
       "2            8     4410      93        142      209       299  \n",
       "3            7     3969      93        142      299       380  \n",
       "4            2     1421      93        142      380       409  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in key with data about words and sentences\n",
    "PTH_KEY = 'C:/6120-NLP/project/GECO/EnglishMaterial.csv'\n",
    "keyset = pd.read_csv(PTH_KEY)\n",
    "keyset['WORD_ID'] = keyset['WORD_ID'].apply(fix_id)\n",
    "keyset['SENTENCE_ID'] = keyset['SENTENCE_ID'].apply(fix_id)\n",
    "\n",
    "keyset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mapping: \n",
      "*******************************************\n",
      "Verb            11963\n",
      "Noun             9356\n",
      "Pronoun          8117\n",
      "Preposition      4553\n",
      "Adverb           4207\n",
      "Article          4032\n",
      "Conjunction      2592\n",
      "Adjective        2561\n",
      "Determiner       2394\n",
      "To               1337\n",
      "Name             1219\n",
      "Interjection      654\n",
      "Number            558\n",
      "Not               530\n",
      "Ex                243\n",
      "Letter             17\n",
      ".                  16\n",
      "Unclassified       12\n",
      "Name: PART_OF_SPEECH, dtype: int64\n",
      "\n",
      "After mapping: \n",
      "*******************************************\n",
      "VERB    11963\n",
      "NOUN    10575\n",
      "PRON     8117\n",
      "DET      6426\n",
      "ADP      5890\n",
      "ADV      4450\n",
      "CONJ     2592\n",
      "ADJ      2561\n",
      "X         699\n",
      "NUM       558\n",
      "PRT       530\n",
      "Name: PART_OF_SPEECH, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Map PART_OF_SPEECH to POS from universal tag set (Petrov et al, 2011) available at:\n",
    "https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "VERB - verbs (all tenses and modes)\n",
    "NOUN - nouns (common and proper)\n",
    "PRON - pronouns \n",
    "ADJ - adjectives\n",
    "ADV - adverbs\n",
    "ADP - adpositions (prepositions and postpositions)\n",
    "CONJ - conjunctions\n",
    "DET - determiners\n",
    "NUM - cardinal numbers\n",
    "PRT - particles or other function words\n",
    "X - other: foreign words, typos, abbreviations\n",
    ". - punctuation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Before mapping: \")\n",
    "print(\"*******************************************\")\n",
    "print(keyset['PART_OF_SPEECH'].value_counts())\n",
    "universal_tags = {'Verb' : 'VERB',\n",
    "                  'Noun' : 'NOUN',\n",
    "                  'Pronoun' : 'PRON',\n",
    "                  'Preposition' : 'ADP',\n",
    "                  'Adverb' : 'ADV',\n",
    "                  'Article' : 'DET',\n",
    "                  'Conjunction' : 'CONJ',\n",
    "                  'Adjective' : 'ADJ',\n",
    "                  'Determiner' : 'DET',\n",
    "                  'To' : 'ADP',\n",
    "                  'Name' : 'NOUN',\n",
    "                  'Interjection' : 'X',\n",
    "                  'Number' : 'NUM',\n",
    "                  'Not' : 'PRT',\n",
    "                  'Ex' : 'ADV',\n",
    "                  'Letter' : 'X',\n",
    "                  '.' : 'X',\n",
    "                  'Unclassified' : 'X'\n",
    "                 }\n",
    "\n",
    "print(\"\\nAfter mapping: \")\n",
    "print(\"*******************************************\")\n",
    "keyset[\"PART_OF_SPEECH\"].replace(universal_tags, inplace=True)\n",
    "print(keyset['PART_OF_SPEECH'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>SENTENCE_ID</th>\n",
       "      <th>CHRON_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>PART_OF_SPEECH</th>\n",
       "      <th>CONTENT_WORD</th>\n",
       "      <th>WORD_LENGTH</th>\n",
       "      <th>IA_AREA</th>\n",
       "      <th>IA_TOP</th>\n",
       "      <th>IA_BOTTOM</th>\n",
       "      <th>IA_LEFT</th>\n",
       "      <th>IA_RIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_5_1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>DET</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3087</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>66</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_5_2</td>\n",
       "      <td>1_1</td>\n",
       "      <td>2</td>\n",
       "      <td>intense</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3920</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_5_3</td>\n",
       "      <td>1_1</td>\n",
       "      <td>3</td>\n",
       "      <td>interest</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4410</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>209</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_5_4</td>\n",
       "      <td>1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3969</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>299</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5_5</td>\n",
       "      <td>1_1</td>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1421</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>380</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WORD_ID SENTENCE_ID  CHRON_ID      WORD PART_OF_SPEECH  CONTENT_WORD  \\\n",
       "0   1_5_1         1_1         1       The            DET             0   \n",
       "1   1_5_2         1_1         2   intense            ADJ             1   \n",
       "2   1_5_3         1_1         3  interest           NOUN             1   \n",
       "3   1_5_4         1_1         4   aroused           VERB             1   \n",
       "4   1_5_5         1_1         5        in            ADP             0   \n",
       "\n",
       "   WORD_LENGTH  IA_AREA  IA_TOP  IA_BOTTOM  IA_LEFT  IA_RIGHT  \n",
       "0            3     3087      93        142       66       129  \n",
       "1            7     3920      93        142      129       209  \n",
       "2            8     4410      93        142      209       299  \n",
       "3            7     3969      93        142      299       380  \n",
       "4            2     1421      93        142      380       409  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5285\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Create a file that we will need for clustering of the form:\n",
    " - each sentence as a line\n",
    " - each word seperated by white space\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "# number of unique sentences that there should be\n",
    "n = len(pd.unique(keyset['SENTENCE_ID']))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "\n",
    "file_name = \"C:/6120-NLP/project/input.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "sentences = open(file_name, \"w\")\n",
    "\n",
    "old = keyset['SENTENCE_ID'][0]\n",
    "line = ''\n",
    "for i in range(len(keyset)):\n",
    "    new = keyset.loc[i,'SENTENCE_ID']\n",
    "    if old == new:\n",
    "        line = line + str(keyset.loc[i, 'WORD']) + ' '\n",
    "    else:\n",
    "        line = line + '\\n'\n",
    "        sentences.write(line)\n",
    "        line = str(keyset.loc[i, 'WORD']) + ' '\n",
    "    old = new\n",
    "    \n",
    "line = line + '\\n'\n",
    "sentences.write(line)\n",
    "sentences.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Get eye movement features as by token and by type. The features used will be:\n",
    "\n",
    "-WORD_FIXATION_%\n",
    "-WORD_FIXATION_COUNT\n",
    "-WORD_GAZE DURATION\n",
    "-WORD_RUN_COUNT\n",
    "-WORD_TOTAL_READING_TIME\n",
    "-WORD_TOTAL_READING_TIME_%\n",
    "-WORD_SKIP\n",
    "-WORD_SPILLOVER\n",
    "-WORD_FIRST_FIXATION_DURATION\n",
    "-WORD_FIRST_FIXATION_DURATION + SECOND + THIRD + LAST / 4 (MEAN)\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>WORD_FIXATION_COUNT</th>\n",
       "      <th>WORD_FIXATION_%</th>\n",
       "      <th>WORD_GAZE_DURATION</th>\n",
       "      <th>WORD_FIRST_FIXATION_DURATION</th>\n",
       "      <th>WORD_SECOND_FIXATION_DURATION</th>\n",
       "      <th>WORD_THIRD_FIXATION_DURATION</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_RUN_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>The</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>intense</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>interest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>333.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR  PART  TRIAL WORD_ID      WORD  WORD_FIXATION_COUNT  WORD_FIXATION_%  \\\n",
       "0  pp21     1      5   1_5_1       The                  2.0           0.0174   \n",
       "1  pp21     1      5   1_5_2   intense                  3.0           0.0261   \n",
       "2  pp21     1      5   1_5_3  interest                  2.0           0.0174   \n",
       "3  pp21     1      5   1_5_4   aroused                  3.0           0.0261   \n",
       "4  pp21     1      5   1_5_5        in                  1.0           0.0087   \n",
       "\n",
       "   WORD_GAZE_DURATION  WORD_FIRST_FIXATION_DURATION  \\\n",
       "0                95.0                          95.0   \n",
       "1                54.0                          54.0   \n",
       "2               333.0                         333.0   \n",
       "3                78.0                          78.0   \n",
       "4               154.0                         154.0   \n",
       "\n",
       "   WORD_SECOND_FIXATION_DURATION  WORD_THIRD_FIXATION_DURATION  \\\n",
       "0                          286.0                           0.0   \n",
       "1                          242.0                         532.0   \n",
       "2                          232.0                           0.0   \n",
       "3                          215.0                         135.0   \n",
       "4                            0.0                           0.0   \n",
       "\n",
       "   WORD_TOTAL_READING_TIME  WORD_TOTAL_READING_TIME_%  WORD_SKIP  \\\n",
       "0                    381.0                     0.0150        0.0   \n",
       "1                    828.0                     0.0326        1.0   \n",
       "2                    565.0                     0.0222        1.0   \n",
       "3                    428.0                     0.0168        0.0   \n",
       "4                    154.0                     0.0061        1.0   \n",
       "\n",
       "   WORD_SPILLOVER  WORD_RUN_COUNT  \n",
       "0             0.0             2.0  \n",
       "1             0.0             3.0  \n",
       "2             0.0             2.0  \n",
       "3             0.0             2.0  \n",
       "4             0.0             1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop some columns that we don't need\n",
    "dataset_filtered = dataset[['PP_NR','PART', 'TRIAL',\n",
    "                           'WORD_ID', 'WORD', 'WORD_FIXATION_COUNT', 'WORD_FIXATION_%',\n",
    "                           'WORD_GAZE_DURATION', 'WORD_FIRST_FIXATION_DURATION', 'WORD_SECOND_FIXATION_DURATION',\n",
    "                           'WORD_THIRD_FIXATION_DURATION', 'WORD_TOTAL_READING_TIME', 'WORD_TOTAL_READING_TIME_%',\n",
    "                           'WORD_SKIP', 'WORD_SPILLOVER', 'WORD_RUN_COUNT']]\n",
    "\n",
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with words that were removed\n",
    "word_ids = keyset['WORD_ID'].tolist()\n",
    "dataset_filtered = dataset_filtered[dataset_filtered['WORD_ID'].isin(word_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean each word so that is does not have punctuation\n",
    "def clean_word(w):\n",
    "    try:\n",
    "        w = re.sub(r'[^\\w\\s]', '', w)\n",
    "    except:\n",
    "        pass\n",
    "    return w\n",
    "dataset_filtered['WORD_CLEAN'] = dataset_filtered['WORD'].apply(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>WORD_FIXATION_COUNT</th>\n",
       "      <th>WORD_FIXATION_%</th>\n",
       "      <th>WORD_GAZE_DURATION</th>\n",
       "      <th>WORD_FIRST_FIXATION_DURATION</th>\n",
       "      <th>WORD_SECOND_FIXATION_DURATION</th>\n",
       "      <th>WORD_THIRD_FIXATION_DURATION</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_RUN_COUNT</th>\n",
       "      <th>WORD_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>The</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>intense</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>intense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>interest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>333.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>aroused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR  PART  TRIAL WORD_ID      WORD  WORD_FIXATION_COUNT  WORD_FIXATION_%  \\\n",
       "0  pp21     1      5   1_5_1       The                  2.0           0.0174   \n",
       "1  pp21     1      5   1_5_2   intense                  3.0           0.0261   \n",
       "2  pp21     1      5   1_5_3  interest                  2.0           0.0174   \n",
       "3  pp21     1      5   1_5_4   aroused                  3.0           0.0261   \n",
       "4  pp21     1      5   1_5_5        in                  1.0           0.0087   \n",
       "\n",
       "   WORD_GAZE_DURATION  WORD_FIRST_FIXATION_DURATION  \\\n",
       "0                95.0                          95.0   \n",
       "1                54.0                          54.0   \n",
       "2               333.0                         333.0   \n",
       "3                78.0                          78.0   \n",
       "4               154.0                         154.0   \n",
       "\n",
       "   WORD_SECOND_FIXATION_DURATION  WORD_THIRD_FIXATION_DURATION  \\\n",
       "0                          286.0                           0.0   \n",
       "1                          242.0                         532.0   \n",
       "2                          232.0                           0.0   \n",
       "3                          215.0                         135.0   \n",
       "4                            0.0                           0.0   \n",
       "\n",
       "   WORD_TOTAL_READING_TIME  WORD_TOTAL_READING_TIME_%  WORD_SKIP  \\\n",
       "0                    381.0                     0.0150        0.0   \n",
       "1                    828.0                     0.0326        1.0   \n",
       "2                    565.0                     0.0222        1.0   \n",
       "3                    428.0                     0.0168        0.0   \n",
       "4                    154.0                     0.0061        1.0   \n",
       "\n",
       "   WORD_SPILLOVER  WORD_RUN_COUNT WORD_CLEAN  \n",
       "0             0.0             2.0        The  \n",
       "1             0.0             3.0    intense  \n",
       "2             0.0             2.0   interest  \n",
       "3             0.0             2.0    aroused  \n",
       "4             0.0             1.0         in  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the filtered features for each participant\n",
    "dataset_filtered.to_csv(\"C:/6120-NLP/project/gaze_features.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over participant, so we have a feature for each token\n",
    "dataset_tokens = dataset_filtered.groupby(['WORD_ID','WORD_CLEAN']).mean().reset_index()\n",
    "dataset_tokens.to_csv(\"C:/6120-NLP/project/token_features.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over each token, so we have type feautres\n",
    "dataset_types = dataset_tokens.groupby(['WORD_CLEAN']).mean().reset_index()\n",
    "dataset_types.to_csv(\"C:/6120-NLP/project/type_features.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a type dictionary and dump to pickle to use in the model\n",
    "# will map word type to np array of eye movement features according to the order in the pd dataframe\n",
    "type_dict = {}\n",
    "for i in range(len(dataset_types)):\n",
    "    f = np.array([dataset_types.loc[i, 'WORD_FIXATION_COUNT'],\n",
    "             dataset_types.loc[i, 'WORD_FIXATION_%'],\n",
    "             dataset_types.loc[i, 'WORD_GAZE_DURATION'],\n",
    "             dataset_types.loc[i, 'WORD_FIRST_FIXATION_DURATION'],\n",
    "             dataset_types.loc[i, 'WORD_SECOND_FIXATION_DURATION'],\n",
    "             dataset_types.loc[i, 'WORD_THIRD_FIXATION_DURATION'],\n",
    "             dataset_types.loc[i, 'WORD_TOTAL_READING_TIME'],\n",
    "             dataset_types.loc[i, 'WORD_TOTAL_READING_TIME_%'],\n",
    "             dataset_types.loc[i, 'WORD_SKIP'],\n",
    "             dataset_types.loc[i, 'WORD_SPILLOVER'],\n",
    "             dataset_types.loc[i, 'WORD_RUN_COUNT']])\n",
    "    \n",
    "    type_dict[dataset_types.loc[i, 'WORD_CLEAN']] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save type dictionary to pickle\n",
    "with open('type_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(type_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
