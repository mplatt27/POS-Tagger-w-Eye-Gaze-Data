{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Pre-processing\n",
    "  author: MP\n",
    "  date: 4/29/2021\n",
    "  \n",
    "  This study uses data from the GECO (Ghent Eye Tracking Corpus) which is available freely for use\n",
    "  at: https://expsy.ugent.be/downloads/geco/.\n",
    "  \n",
    "  The following code includes several steps for pre-processing the data and creating input files needed\n",
    "  for different parts of the experiments. \n",
    "  \n",
    "  0. Formatting pandas dataframe and changing data to types that we need\n",
    "  1. Map GECO POS tags to Universal Tag set (Petrov et al, 2011)\n",
    "  2. Set up data set that is needed to create eye gaze features. \n",
    "  3. Create data file in format needed for Brown clustering\n",
    "  4. Create data files in the format that we need for MINITAGGER (Stratos and Collins, 2015)\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>LANGUAGE_RANK</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>TRIAL_FIXATION_COUNT</th>\n",
       "      <th>TRIAL_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_ID_WITHIN_TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>WORD_LAST_FIXATION_RUN</th>\n",
       "      <th>WORD_LAST_FIXATION_TIME</th>\n",
       "      <th>WORD_LAST_FIXATION_X</th>\n",
       "      <th>WORD_LAST_FIXATION_Y</th>\n",
       "      <th>WORD_GO_PAST_TIME</th>\n",
       "      <th>WORD_SELECTIVE_GO_PAST_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>1</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>357</td>\n",
       "      <td>115.6</td>\n",
       "      <td>104.6</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>2</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1392</td>\n",
       "      <td>163.8</td>\n",
       "      <td>107</td>\n",
       "      <td>582</td>\n",
       "      <td>296</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>3</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1957</td>\n",
       "      <td>229.4</td>\n",
       "      <td>96.6</td>\n",
       "      <td>1097</td>\n",
       "      <td>565</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>4</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2474</td>\n",
       "      <td>356</td>\n",
       "      <td>103.8</td>\n",
       "      <td>2107</td>\n",
       "      <td>428</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>monolingual</td>\n",
       "      <td>L1</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>25429</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2808</td>\n",
       "      <td>403.2</td>\n",
       "      <td>114.5</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR        GROUP LANGUAGE_RANK LANGUAGE  PART  TRIAL  \\\n",
       "0  pp21  monolingual            L1  English     1      5   \n",
       "1  pp21  monolingual            L1  English     1      5   \n",
       "2  pp21  monolingual            L1  English     1      5   \n",
       "3  pp21  monolingual            L1  English     1      5   \n",
       "4  pp21  monolingual            L1  English     1      5   \n",
       "\n",
       "   TRIAL_FIXATION_COUNT  TRIAL_TOTAL_READING_TIME  WORD_ID_WITHIN_TRIAL  \\\n",
       "0                   115                     25429                     1   \n",
       "1                   115                     25429                     2   \n",
       "2                   115                     25429                     3   \n",
       "3                   115                     25429                     4   \n",
       "4                   115                     25429                     5   \n",
       "\n",
       "  WORD_ID  ... WORD_LAST_FIXATION_RUN WORD_LAST_FIXATION_TIME  \\\n",
       "0   1_5_1  ...                      2                     357   \n",
       "1   1_5_2  ...                      3                    1392   \n",
       "2   1_5_3  ...                      2                    1957   \n",
       "3   1_5_4  ...                      2                    2474   \n",
       "4   1_5_5  ...                      1                    2808   \n",
       "\n",
       "   WORD_LAST_FIXATION_X  WORD_LAST_FIXATION_Y  WORD_GO_PAST_TIME  \\\n",
       "0                 115.6                 104.6                 95   \n",
       "1                 163.8                   107                582   \n",
       "2                 229.4                  96.6               1097   \n",
       "3                   356                 103.8               2107   \n",
       "4                 403.2                 114.5                154   \n",
       "\n",
       "  WORD_SELECTIVE_GO_PAST_TIME WORD_TOTAL_READING_TIME  \\\n",
       "0                          95                   381.0   \n",
       "1                         296                   828.0   \n",
       "2                         565                   565.0   \n",
       "3                         428                   428.0   \n",
       "4                         154                   154.0   \n",
       "\n",
       "  WORD_TOTAL_READING_TIME_% WORD_SPILLOVER  WORD_SKIP  \n",
       "0                    0.0150            0.0        0.0  \n",
       "1                    0.0326            0.0        1.0  \n",
       "2                    0.0222            0.0        1.0  \n",
       "3                    0.0168            0.0        0.0  \n",
       "4                    0.0061            0.0        1.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "0. Format the data in the pandas dataframes\n",
    "\"\"\"\n",
    "\n",
    "# read in the dataset with eye tracking data\n",
    "PTH_DATA = 'C:/6120-NLP/project/third-party/GECO/MonolingualReadingData.csv'\n",
    "dataset = pd.read_csv(PTH_DATA)\n",
    "\n",
    "\n",
    "def fix_id(w):\n",
    "    \"\"\" Replace - with _ to avoid excel date reformatting. \"\"\"\n",
    "    w = str(w)\n",
    "    w = w.replace(\"-\", \"_\")\n",
    "    return w\n",
    "\n",
    "\n",
    "def fix_nulls(c):\n",
    "    \"\"\" Replace null values in cells with empty string; replace numbers with floats. \"\"\"\n",
    "    if c == \".\":\n",
    "        c = 0\n",
    "    return float(c)\n",
    "\n",
    "# run changes\n",
    "dataset['WORD_ID'] = dataset['WORD_ID'].apply(fix_id)\n",
    "dataset['WORD_FIXATION_%'] = dataset['WORD_FIXATION_%'].apply(fix_nulls)\n",
    "dataset['WORD_FIXATION_COUNT'] = dataset['WORD_FIXATION_COUNT'].apply(fix_nulls)\n",
    "dataset['WORD_GAZE_DURATION'] = dataset['WORD_GAZE_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_RUN_COUNT'] = dataset['WORD_RUN_COUNT'].apply(fix_nulls)\n",
    "dataset['WORD_SKIP'] = dataset['WORD_SKIP'].apply(fix_nulls)\n",
    "dataset['WORD_FIRST_FIXATION_DURATION'] = dataset['WORD_FIRST_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_SECOND_FIXATION_DURATION'] = dataset['WORD_SECOND_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_THIRD_FIXATION_DURATION'] = dataset['WORD_THIRD_FIXATION_DURATION'].apply(fix_nulls)\n",
    "dataset['WORD_TOTAL_READING_TIME'] = dataset['WORD_TOTAL_READING_TIME'].apply(fix_nulls)\n",
    "dataset['WORD_TOTAL_READING_TIME_%'] = dataset['WORD_TOTAL_READING_TIME_%'].apply(fix_nulls)\n",
    "dataset['WORD_SPILLOVER'] = dataset['WORD_SPILLOVER'].apply(fix_nulls)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>SENTENCE_ID</th>\n",
       "      <th>CHRON_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>PART_OF_SPEECH</th>\n",
       "      <th>CONTENT_WORD</th>\n",
       "      <th>WORD_LENGTH</th>\n",
       "      <th>IA_AREA</th>\n",
       "      <th>IA_TOP</th>\n",
       "      <th>IA_BOTTOM</th>\n",
       "      <th>IA_LEFT</th>\n",
       "      <th>IA_RIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_5_1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>Article</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3087</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>66</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_5_2</td>\n",
       "      <td>1_1</td>\n",
       "      <td>2</td>\n",
       "      <td>intense</td>\n",
       "      <td>Adjective</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3920</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_5_3</td>\n",
       "      <td>1_1</td>\n",
       "      <td>3</td>\n",
       "      <td>interest</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4410</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>209</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_5_4</td>\n",
       "      <td>1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>Verb</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3969</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>299</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5_5</td>\n",
       "      <td>1_1</td>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "      <td>Preposition</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1421</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>380</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WORD_ID SENTENCE_ID  CHRON_ID      WORD PART_OF_SPEECH  CONTENT_WORD  \\\n",
       "0   1_5_1         1_1         1       The        Article             0   \n",
       "1   1_5_2         1_1         2   intense      Adjective             1   \n",
       "2   1_5_3         1_1         3  interest           Noun             1   \n",
       "3   1_5_4         1_1         4   aroused           Verb             1   \n",
       "4   1_5_5         1_1         5        in    Preposition             0   \n",
       "\n",
       "   WORD_LENGTH  IA_AREA  IA_TOP  IA_BOTTOM  IA_LEFT  IA_RIGHT  \n",
       "0            3     3087      93        142       66       129  \n",
       "1            7     3920      93        142      129       209  \n",
       "2            8     4410      93        142      209       299  \n",
       "3            7     3969      93        142      299       380  \n",
       "4            2     1421      93        142      380       409  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in key with data about words and sentences\n",
    "PTH_KEY = 'C:/6120-NLP/project/third-party/GECO/EnglishMaterial.csv'\n",
    "keyset = pd.read_csv(PTH_KEY)\n",
    "keyset['WORD_ID'] = keyset['WORD_ID'].apply(fix_id)\n",
    "keyset['SENTENCE_ID'] = keyset['SENTENCE_ID'].apply(fix_id)\n",
    "\n",
    "keyset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mapping: \n",
      "*******************************************\n",
      "Verb            11963\n",
      "Noun             9356\n",
      "Pronoun          8117\n",
      "Preposition      4553\n",
      "Adverb           4207\n",
      "Article          4032\n",
      "Conjunction      2592\n",
      "Adjective        2561\n",
      "Determiner       2394\n",
      "To               1337\n",
      "Name             1219\n",
      "Interjection      654\n",
      "Number            558\n",
      "Not               530\n",
      "Ex                243\n",
      "Letter             17\n",
      ".                  16\n",
      "Unclassified       12\n",
      "Name: PART_OF_SPEECH, dtype: int64\n",
      "\n",
      "After mapping: \n",
      "*******************************************\n",
      "VERB    11963\n",
      "NOUN    10575\n",
      "PRON     8117\n",
      "DET      6426\n",
      "ADP      5890\n",
      "ADV      4450\n",
      "CONJ     2592\n",
      "ADJ      2561\n",
      "X         699\n",
      "NUM       558\n",
      "PRT       530\n",
      "Name: PART_OF_SPEECH, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Map PART_OF_SPEECH to POS from universal tag set (Petrov et al, 2011) available at:\n",
    "https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "VERB - verbs (all tenses and modes)\n",
    "NOUN - nouns (common and proper)\n",
    "PRON - pronouns \n",
    "ADJ - adjectives\n",
    "ADV - adverbs\n",
    "ADP - adpositions (prepositions and postpositions)\n",
    "CONJ - conjunctions\n",
    "DET - determiners\n",
    "NUM - cardinal numbers\n",
    "PRT - particles or other function words\n",
    "X - other: foreign words, typos, abbreviations\n",
    ". - punctuation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Before mapping: \")\n",
    "print(\"*******************************************\")\n",
    "print(keyset['PART_OF_SPEECH'].value_counts())\n",
    "universal_tags = {'Verb' : 'VERB',\n",
    "                  'Noun' : 'NOUN',\n",
    "                  'Pronoun' : 'PRON',\n",
    "                  'Preposition' : 'ADP',\n",
    "                  'Adverb' : 'ADV',\n",
    "                  'Article' : 'DET',\n",
    "                  'Conjunction' : 'CONJ',\n",
    "                  'Adjective' : 'ADJ',\n",
    "                  'Determiner' : 'DET',\n",
    "                  'To' : 'ADP',\n",
    "                  'Name' : 'NOUN',\n",
    "                  'Interjection' : 'X',\n",
    "                  'Number' : 'NUM',\n",
    "                  'Not' : 'PRT',\n",
    "                  'Ex' : 'ADV',\n",
    "                  'Letter' : 'X',\n",
    "                  '.' : 'X',\n",
    "                  'Unclassified' : 'X'\n",
    "                 }\n",
    "\n",
    "print(\"\\nAfter mapping: \")\n",
    "print(\"*******************************************\")\n",
    "keyset[\"PART_OF_SPEECH\"].replace(universal_tags, inplace=True)\n",
    "print(keyset['PART_OF_SPEECH'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>SENTENCE_ID</th>\n",
       "      <th>CHRON_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>PART_OF_SPEECH</th>\n",
       "      <th>CONTENT_WORD</th>\n",
       "      <th>WORD_LENGTH</th>\n",
       "      <th>IA_AREA</th>\n",
       "      <th>IA_TOP</th>\n",
       "      <th>IA_BOTTOM</th>\n",
       "      <th>IA_LEFT</th>\n",
       "      <th>IA_RIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_5_1</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>DET</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3087</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>66</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_5_2</td>\n",
       "      <td>1_1</td>\n",
       "      <td>2</td>\n",
       "      <td>intense</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3920</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_5_3</td>\n",
       "      <td>1_1</td>\n",
       "      <td>3</td>\n",
       "      <td>interest</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4410</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>209</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_5_4</td>\n",
       "      <td>1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3969</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>299</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_5_5</td>\n",
       "      <td>1_1</td>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1421</td>\n",
       "      <td>93</td>\n",
       "      <td>142</td>\n",
       "      <td>380</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  WORD_ID SENTENCE_ID  CHRON_ID      WORD PART_OF_SPEECH  CONTENT_WORD  \\\n",
       "0   1_5_1         1_1         1       The            DET             0   \n",
       "1   1_5_2         1_1         2   intense            ADJ             1   \n",
       "2   1_5_3         1_1         3  interest           NOUN             1   \n",
       "3   1_5_4         1_1         4   aroused           VERB             1   \n",
       "4   1_5_5         1_1         5        in            ADP             0   \n",
       "\n",
       "   WORD_LENGTH  IA_AREA  IA_TOP  IA_BOTTOM  IA_LEFT  IA_RIGHT  \n",
       "0            3     3087      93        142       66       129  \n",
       "1            7     3920      93        142      129       209  \n",
       "2            8     4410      93        142      209       299  \n",
       "3            7     3969      93        142      299       380  \n",
       "4            2     1421      93        142      380       409  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keyset to csv\n",
    "keyset.to_csv(\"C:/6120-NLP/project/keyset.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Set up data that we will need to create eye gaze features later. We will want: \n",
    "\n",
    "-WORD_FIXATION_%\n",
    "-WORD_FIXATION_COUNT\n",
    "-WORD_GAZE DURATION\n",
    "-WORD_RUN_COUNT\n",
    "-WORD_TOTAL_READING_TIME\n",
    "-WORD_TOTAL_READING_TIME_%\n",
    "-WORD_SKIP\n",
    "-WORD_SPILLOVER\n",
    "-WORD_FIRST_FIXATION_DURATION\n",
    "-WORD_FIRST_FIXATION_DURATION + SECOND + THIRD + LAST / 4 (MEAN)\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>WORD_FIXATION_COUNT</th>\n",
       "      <th>WORD_FIXATION_%</th>\n",
       "      <th>WORD_GAZE_DURATION</th>\n",
       "      <th>WORD_FIRST_FIXATION_DURATION</th>\n",
       "      <th>WORD_SECOND_FIXATION_DURATION</th>\n",
       "      <th>WORD_THIRD_FIXATION_DURATION</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_RUN_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>The</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>intense</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>interest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>333.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR  PART  TRIAL WORD_ID      WORD  WORD_FIXATION_COUNT  WORD_FIXATION_%  \\\n",
       "0  pp21     1      5   1_5_1       The                  2.0           0.0174   \n",
       "1  pp21     1      5   1_5_2   intense                  3.0           0.0261   \n",
       "2  pp21     1      5   1_5_3  interest                  2.0           0.0174   \n",
       "3  pp21     1      5   1_5_4   aroused                  3.0           0.0261   \n",
       "4  pp21     1      5   1_5_5        in                  1.0           0.0087   \n",
       "\n",
       "   WORD_GAZE_DURATION  WORD_FIRST_FIXATION_DURATION  \\\n",
       "0                95.0                          95.0   \n",
       "1                54.0                          54.0   \n",
       "2               333.0                         333.0   \n",
       "3                78.0                          78.0   \n",
       "4               154.0                         154.0   \n",
       "\n",
       "   WORD_SECOND_FIXATION_DURATION  WORD_THIRD_FIXATION_DURATION  \\\n",
       "0                          286.0                           0.0   \n",
       "1                          242.0                         532.0   \n",
       "2                          232.0                           0.0   \n",
       "3                          215.0                         135.0   \n",
       "4                            0.0                           0.0   \n",
       "\n",
       "   WORD_TOTAL_READING_TIME  WORD_TOTAL_READING_TIME_%  WORD_SKIP  \\\n",
       "0                    381.0                     0.0150        0.0   \n",
       "1                    828.0                     0.0326        1.0   \n",
       "2                    565.0                     0.0222        1.0   \n",
       "3                    428.0                     0.0168        0.0   \n",
       "4                    154.0                     0.0061        1.0   \n",
       "\n",
       "   WORD_SPILLOVER  WORD_RUN_COUNT  \n",
       "0             0.0             2.0  \n",
       "1             0.0             3.0  \n",
       "2             0.0             2.0  \n",
       "3             0.0             2.0  \n",
       "4             0.0             1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop some columns that we don't need\n",
    "dataset_filtered = dataset[['PP_NR','PART', 'TRIAL',\n",
    "                           'WORD_ID', 'WORD', 'WORD_FIXATION_COUNT', 'WORD_FIXATION_%',\n",
    "                           'WORD_GAZE_DURATION', 'WORD_FIRST_FIXATION_DURATION', 'WORD_SECOND_FIXATION_DURATION',\n",
    "                           'WORD_THIRD_FIXATION_DURATION', 'WORD_TOTAL_READING_TIME', 'WORD_TOTAL_READING_TIME_%',\n",
    "                           'WORD_SKIP', 'WORD_SPILLOVER', 'WORD_RUN_COUNT']]\n",
    "\n",
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with words that were removed\n",
    "word_ids = keyset['WORD_ID'].tolist()\n",
    "dataset_filtered = dataset_filtered[dataset_filtered['WORD_ID'].isin(word_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean each word so that is does not have punctuation\n",
    "def clean_word(w):\n",
    "    try:\n",
    "        w = re.sub(r'[^\\w\\s]', '', w)\n",
    "    except:\n",
    "        pass\n",
    "    return w\n",
    "dataset_filtered['WORD_CLEAN'] = dataset_filtered['WORD'].apply(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PP_NR</th>\n",
       "      <th>PART</th>\n",
       "      <th>TRIAL</th>\n",
       "      <th>WORD_ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>WORD_FIXATION_COUNT</th>\n",
       "      <th>WORD_FIXATION_%</th>\n",
       "      <th>WORD_GAZE_DURATION</th>\n",
       "      <th>WORD_FIRST_FIXATION_DURATION</th>\n",
       "      <th>WORD_SECOND_FIXATION_DURATION</th>\n",
       "      <th>WORD_THIRD_FIXATION_DURATION</th>\n",
       "      <th>WORD_TOTAL_READING_TIME</th>\n",
       "      <th>WORD_TOTAL_READING_TIME_%</th>\n",
       "      <th>WORD_SKIP</th>\n",
       "      <th>WORD_SPILLOVER</th>\n",
       "      <th>WORD_RUN_COUNT</th>\n",
       "      <th>WORD_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_1</td>\n",
       "      <td>The</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_2</td>\n",
       "      <td>intense</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>intense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_3</td>\n",
       "      <td>interest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>333.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_4</td>\n",
       "      <td>aroused</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>aroused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pp21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_5_5</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PP_NR  PART  TRIAL WORD_ID      WORD  WORD_FIXATION_COUNT  WORD_FIXATION_%  \\\n",
       "0  pp21     1      5   1_5_1       The                  2.0           0.0174   \n",
       "1  pp21     1      5   1_5_2   intense                  3.0           0.0261   \n",
       "2  pp21     1      5   1_5_3  interest                  2.0           0.0174   \n",
       "3  pp21     1      5   1_5_4   aroused                  3.0           0.0261   \n",
       "4  pp21     1      5   1_5_5        in                  1.0           0.0087   \n",
       "\n",
       "   WORD_GAZE_DURATION  WORD_FIRST_FIXATION_DURATION  \\\n",
       "0                95.0                          95.0   \n",
       "1                54.0                          54.0   \n",
       "2               333.0                         333.0   \n",
       "3                78.0                          78.0   \n",
       "4               154.0                         154.0   \n",
       "\n",
       "   WORD_SECOND_FIXATION_DURATION  WORD_THIRD_FIXATION_DURATION  \\\n",
       "0                          286.0                           0.0   \n",
       "1                          242.0                         532.0   \n",
       "2                          232.0                           0.0   \n",
       "3                          215.0                         135.0   \n",
       "4                            0.0                           0.0   \n",
       "\n",
       "   WORD_TOTAL_READING_TIME  WORD_TOTAL_READING_TIME_%  WORD_SKIP  \\\n",
       "0                    381.0                     0.0150        0.0   \n",
       "1                    828.0                     0.0326        1.0   \n",
       "2                    565.0                     0.0222        1.0   \n",
       "3                    428.0                     0.0168        0.0   \n",
       "4                    154.0                     0.0061        1.0   \n",
       "\n",
       "   WORD_SPILLOVER  WORD_RUN_COUNT WORD_CLEAN  \n",
       "0             0.0             2.0        The  \n",
       "1             0.0             3.0    intense  \n",
       "2             0.0             2.0   interest  \n",
       "3             0.0             2.0    aroused  \n",
       "4             0.0             1.0         in  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the filtered features for each participant\n",
    "dataset_filtered.to_csv(\"C:/6120-NLP/project/gaze_features.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5285\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. Create a file that we will need for clustering of the form:\n",
    " - each sentence as a line\n",
    " - each word seperated by white space\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "# number of unique sentences that there should be\n",
    "n = len(pd.unique(keyset['SENTENCE_ID']))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n"
     ]
    }
   ],
   "source": [
    "# determine the frequency of each word\n",
    "values = dataset_filtered['WORD_CLEAN'].value_counts().keys().tolist()\n",
    "counts = dataset_filtered['WORD_CLEAN'].value_counts().tolist()\n",
    "\n",
    "low_f_set = set([])\n",
    "for i in range(len(values)):\n",
    "    if counts[i] <= 13:\n",
    "        low_f_set.add(values[i])\n",
    "print(len(low_f_set))\n",
    "\n",
    "\n",
    "dataset_filtered_unk = dataset_filtered.copy(deep=True)\n",
    "keyset_unk = keyset.copy(deep=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 682 words with a frequency of 13 or lower, we will replace them with UNK <?> symbol\n",
    "for i, r in dataset_filtered_unk.iterrows():\n",
    "    if dataset_filtered_unk.at[i, 'WORD_CLEAN'] in low_f_set:\n",
    "        dataset_filtered_unk.at[i, 'WORD_CLEAN'] = '<?>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in keyset_unk.iterrows():\n",
    "    if keyset_unk.at[i, 'WORD'] in low_f_set:\n",
    "        keyset_unk.at[i, 'WORD'] = '<?>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('<?>' in pd.unique(dataset_filtered_unk['WORD_CLEAN']))\n",
    "print('<?>' in pd.unique(keyset_unk['WORD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file\n",
    "\n",
    "file_name = \"C:/6120-NLP/project/input.txt\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "sentences = open(file_name, \"w\")\n",
    "\n",
    "old = keyset_unk['SENTENCE_ID'][0]\n",
    "line = ''\n",
    "for i in range(len(keyset_unk)):\n",
    "    new = keyset_unk.loc[i,'SENTENCE_ID']\n",
    "    if old == new:\n",
    "        line = line + str(keyset_unk.loc[i, 'WORD']) + ' '\n",
    "    else:\n",
    "        line = line + '\\n'\n",
    "        sentences.write(line)\n",
    "        line = str(keyset_unk.loc[i, 'WORD']) + ' '\n",
    "    old = new\n",
    "    \n",
    "line = line + '\\n'\n",
    "sentences.write(line)\n",
    "sentences.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets with unknown chars\n",
    "dataset_filtered_unk.to_csv(\"C:/6120-NLP/project/dataset_filtered_unk.csv\", index = False, header=True)\n",
    "keyset_unk.to_csv(\"C:/6120-NLP/project/keyset_unk.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".\n",
    "4. Create train, dev, and test files in the format that we need for MINITAGGER (Stratos and Collins, 2015).\n",
    "\n",
    "Sources for MINITAGGER:\n",
    "    Paper: https://www.aclweb.org/anthology/W15-1511.pdf\n",
    "    Code: https://github.com/karlstratos/minitagger\n",
    "    \n",
    "Input train, dev, and test files format\n",
    "\n",
    "Each tag\n",
    "word tag\n",
    "in tag\n",
    "sequence tag\n",
    "\n",
    "Next tag\n",
    "word tag\n",
    "in tag\n",
    "sequence tag\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two parallel lists\n",
    "# sequences = [sequence, sequence] where sequence is list tokenized by words in sequence\n",
    "# labels = [lables, lables] where labels are lists of each label for the word in sentence\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "old = keyset_unk['SENTENCE_ID'][0]\n",
    "line = []\n",
    "line_labels = []\n",
    "for i in range(len(keyset_unk)):\n",
    "    new = keyset_unk.loc[i,'SENTENCE_ID']\n",
    "    if old == new:\n",
    "        line.append(str(keyset_unk.loc[i, 'WORD']))\n",
    "        line_labels.append(str(keyset_unk.loc[i, 'PART_OF_SPEECH']))\n",
    "    else:\n",
    "        sequences.append(line)\n",
    "        labels.append(line_labels)\n",
    "        line = []\n",
    "        line_labels = []\n",
    "        line.append(str(keyset_unk.loc[i, 'WORD']))\n",
    "        line_labels.append(str(keyset_unk.loc[i, 'PART_OF_SPEECH']))\n",
    "    old = new\n",
    "    \n",
    "sequences.append(line)\n",
    "labels.append(line_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "intense ADJ\n",
      "interest NOUN\n",
      "aroused VERB\n",
      "in ADP\n",
      "the DET\n",
      "public NOUN\n",
      "by ADP\n",
      "what DET\n",
      "was VERB\n",
      "known VERB\n",
      "at ADP\n",
      "the DET\n",
      "time NOUN\n",
      "as CONJ\n",
      "The DET\n",
      "Styles NOUN\n",
      "Case NOUN\n",
      "has VERB\n",
      "now ADV\n",
      "somewhat ADV\n",
      "subsided VERB\n"
     ]
    }
   ],
   "source": [
    "# check the some sentences\n",
    "for i in range(1):\n",
    "    s = sequences[i]\n",
    "    l = labels[i]\n",
    "    for j in range(len(s)):\n",
    "        print(s[j], l[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test, and dev sets\n",
    "train_sequences, test_sequences, train_labels, test_labels  = train_test_split(sequences, labels, \n",
    "                                                                               test_size=0.1, shuffle=False)\n",
    "\n",
    "train_sequences, dev_sequences, train_labels, dev_labels = train_test_split(train_sequences, \n",
    "                                                                            train_labels, test_size=0.11, \n",
    "                                                                            shuffle=False) # 0.11 x 0.9 = ~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234 4234\n",
      "524 524\n",
      "529 529\n"
     ]
    }
   ],
   "source": [
    "# check that everything split correctly\n",
    "print(len(train_sequences), len(train_labels))\n",
    "print(len(dev_sequences), len(dev_labels))\n",
    "print(len(test_sequences), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "intense ADJ\n",
      "interest NOUN\n",
      "aroused VERB\n",
      "in ADP\n",
      "the DET\n",
      "public NOUN\n",
      "by ADP\n",
      "what DET\n",
      "was VERB\n",
      "known VERB\n",
      "at ADP\n",
      "the DET\n",
      "time NOUN\n",
      "as CONJ\n",
      "The DET\n",
      "Styles NOUN\n",
      "Case NOUN\n",
      "has VERB\n",
      "now ADV\n",
      "somewhat ADV\n",
      "subsided VERB\n"
     ]
    }
   ],
   "source": [
    "# print to double check\n",
    "s = train_sequences[0]\n",
    "l = train_labels[0]\n",
    "for j in range(len(s)):\n",
    "    print(s[j], l[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this information into files in the format that we need\n",
    "\n",
    "def write_minitagger_input(d, l, file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    o = open(file_name, \"w\")\n",
    "    \n",
    "    for i in range(len(d)):\n",
    "        for j in range(len(d[i])):\n",
    "            o.write(str(d[i][j]) + \" \" + l[i][j] + \"\\n\")\n",
    "        o.write(\"\\n\") \n",
    "    o.close()\n",
    "    \n",
    "write_minitagger_input(train_sequences, train_labels, \"C:/6120-NLP/project/geco_train.txt\")\n",
    "write_minitagger_input(dev_sequences, dev_labels, \"C:/6120-NLP/project/geco_dev.txt\")\n",
    "write_minitagger_input(test_sequences, test_labels, \"C:/6120-NLP/project/geco_test.txt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
